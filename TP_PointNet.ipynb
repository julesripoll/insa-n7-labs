{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julesripoll/insa-n7-labs/blob/main/TP_PointNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI37GkMS-t9d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Aujourd'hui, vous êtes dans la peau d'un des Data Scientists les plus renommés du pays. Vous travaillez pour une entreprise internationale de sécurité. Sa mission est de reconnaître les avions circulant dans l'espace aérien du centre spatial de Toulouse Matabiau.\n",
        "\n",
        "Vous êtes chapeauté par le docteur Henri Muller et faites partie d'une équipe faisant elle-même partie d'un groupe de trois équipes chargées de mettre au point une nouvelle méthode de reconnaissance des avions, basée sur l'utilisation de nuages de points.\n",
        "\n",
        "La première équipe a pour mission de réaliser la segmentation sémantique de scans de l'espace aérien afin d'isoler les avions du reste du nuage de points. Elle a depuis longtemps obtenu des résultats très satisfaisants, qui constituent votre base de données.\n",
        "\n",
        "La seconde autre équipe a déjà obtenu des résultats très probants sur la reconnaissance d'avion grâce à l'étude de ses parties (taille des ailes, nombre de moteurs, etc.). \n",
        "\n",
        "Quand à votre équipe, son rôle est d'assurer la découpage en parties de l'avion à partir de son nuage de points. Mais elle est à la traine... Est-ce la faute aux nombreux afterworks un peu trop arrosés de ces derniers mois ? (/!\\ Ce sujet de TP a été écrit avant la pandémie de Covid-19 et les mesures sanitaires qui ont suivi, et ne consiste en aucun cas à une incitation à braver les directives gouvernementales. Appliquez les gestes barrières.) Vous ne préférez pas y penser. En effet, la deadline est demain. \n",
        "\n",
        "Votre travail devrait ainsi permettre de faire la liaison entre les travaux de la première et de la seconde équipe et ainsi de terminer ce projet essentiel dans le développement de votre entreprise.\n",
        "\n",
        "Après une petite recherche bibliographique, votre équipe a découvert une approche par apprentissage profond qui semble prometteuse : le réseau de neurones PointNet. En effet, selon ses auteurs, il est très performant dans le découpage en parties de formes 3D représentées par un nuage de points.\n",
        "![PointNet](http://stanford.edu/~rqi/pointnet/images/teaser.jpg)\n",
        "\n",
        "L'architecture du modèle complet est illustrée ci-dessous.\n",
        "![Réseaux](http://stanford.edu/~rqi/pointnet/images/pointnet.jpg)\n",
        "\n",
        "Vous avez donc passé quelques jours à annoter péniblement des nuages de points afin de constituer une base d'apprentissage.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLoQ4EUc-DFJ"
      },
      "source": [
        "# ATTENTION \n",
        " Pour le bon déroulement du TP, il vous faut enlever le GPU sinon votre RAM ne va pas supporter la quantité de données !!\n",
        " Allez dans Execution > Modifier le type d'execution > Sélectionner None "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMkkxCcI999I"
      },
      "source": [
        "# Chargement de la base d'apprentissage\n",
        "Commencez d'abord par ajouter le répertoire contenant le dataset à votre Drive :\n",
        "\n",
        "https://drive.google.com/open?id=10KTcmOFkhNcGVwWBKHzzinK5Y6OjsNO3\n",
        "\n",
        "\n",
        "\n",
        "Montez ensuite votre répertoire Google Drive afin de pouvoir y accéder :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVjmLKWRDag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5806161f-363d-448e-de17-707716b54994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbs2PFI_-fk_"
      },
      "source": [
        "# ATTENTION\n",
        "La ligne de code suivante est à DECOMMENTER une seule fois.\n",
        "Ensuite il faut le RECOMMENTER.\n",
        "Cette ligne vous permet d'éviter cette erreur lors du chargement du submodel : \n",
        "\n",
        "\"AttributeError: 'str' object has no attribute 'decode'\"\n",
        "\n",
        "L'execution peut finir par une erreur mais ce n'est pas grave tant que le package h5py a été installé sous sa version 2.10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EENgQ-k-tap"
      },
      "outputs": [],
      "source": [
        "#!pip install h5py==2.10.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ofgV0vPmzI"
      },
      "source": [
        "Chargez ensuite les fichiers *airplane_train.h5* contenant les données d'entraînement et *airplane_test.h5* contenant les données de test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5j5GnFKfvtNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6dbdd3-e2e2-4200-ac0e-f1d1df76d35f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def load_h5(h5_filename):\n",
        "    f = h5py.File(h5_filename)\n",
        "    data = f['data'][:]\n",
        "    label = f['label'][:]\n",
        "    return (data, label)\n",
        "\n",
        "# Nombre de point par échantillon\n",
        "num_points = 1024\n",
        "\n",
        "# Nombre de catégories\n",
        "k = 4\n",
        "\n",
        "# Chargement des données d'entraînement\n",
        "filename = '/content/drive/My Drive/PointNet/airplane_train.h5'\n",
        "train_points, train_labels = load_h5(filename)\n",
        "train_points = train_points.reshape(-1, num_points, 3)\n",
        "train_labels = train_labels.reshape(-1, num_points, k)\n",
        "\n",
        "# Chargement des données de test\n",
        "filename = '/content/drive/My Drive/PointNet/airplane_test.h5'\n",
        "test_points, test_labels = load_h5(filename)\n",
        "test_points = test_points.reshape(-1, num_points, 3)\n",
        "test_labels = test_labels.reshape(-1, num_points, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEiISbZXPSkk"
      },
      "source": [
        "# Parce que c'est votre projet !\n",
        "\n",
        "Il n'y a plus qu'à implémenter et entraîner le réseau ! Un de vos collègues s'est déjà chargé de la partie classification du modèle (i.e. la partie avec le fond bleu), cependant il vient d'aller se coucher, épuisé après ses deux heures de travail hebdomadaires.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx1zAinlI_MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4edf8c01-c372-47ec-8381-d6a7bc8c85cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "from keras.layers import Input\n",
        "from keras.layers import Convolution1D, BatchNormalization, MaxPooling1D\n",
        "from keras.layers import Dense, Reshape\n",
        "from keras.layers import Lambda, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def mat_mul(A, B):\n",
        "    return tf.matmul(A, B)\n",
        "\n",
        "\n",
        "def exp_dim(global_feature, num_points):\n",
        "    return tf.tile(global_feature, [1, num_points, 1])\n",
        "\n",
        "# Modèle pour la classification + concatenation des features locales et globales\n",
        "def get_submodel():\n",
        "  # Input Transformation Net\n",
        "  input_points = Input(shape=(num_points, 3))\n",
        "  # T-Net\n",
        "  x = Convolution1D(64, 1, activation='relu',\n",
        "                  input_shape=(num_points, 3))(input_points)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Convolution1D(128, 1, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Convolution1D(1024, 1, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling1D(pool_size=num_points)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # 3*3 Transform\n",
        "  x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
        "  input_T = Reshape((3, 3))(x)\n",
        "\n",
        "  # Multiplication matricielle\n",
        "  g = Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
        "\n",
        "  # MLP(64,64)\n",
        "  g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
        "  g = BatchNormalization()(g)\n",
        "  g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
        "  g = BatchNormalization()(g)\n",
        "\n",
        "  # Feature Transformation Net\n",
        "  # T-Net\n",
        "  f = Convolution1D(64, 1, activation='relu')(g)\n",
        "  f = BatchNormalization()(f)\n",
        "  f = Convolution1D(128, 1, activation='relu')(f)\n",
        "  f = BatchNormalization()(f)\n",
        "  f = Convolution1D(1024, 1, activation='relu')(f)\n",
        "  f = BatchNormalization()(f)\n",
        "  f = MaxPooling1D(pool_size=num_points)(f)\n",
        "  f = Dense(512, activation='relu')(f)\n",
        "  f = BatchNormalization()(f)\n",
        "  f = Dense(256, activation='relu')(f)\n",
        "  f = BatchNormalization()(f)\n",
        "\n",
        "  # 64*64 Transform\n",
        "  f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
        "  feature_T = Reshape((64, 64))(f)\n",
        "\n",
        "  # Multiplication matricielle\n",
        "  g = Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
        "  seg_part1 = g\n",
        "\n",
        "  # MLP(64,128,1024)\n",
        "  g = Convolution1D(64, 1, activation='relu')(g)\n",
        "  g = BatchNormalization()(g)\n",
        "  g = Convolution1D(128, 1, activation='relu')(g)\n",
        "  g = BatchNormalization()(g)\n",
        "  g = Convolution1D(1024, 1, activation='relu')(g)\n",
        "  g = BatchNormalization()(g)\n",
        "\n",
        "  # Global Feature\n",
        "  global_feature = MaxPooling1D(pool_size=num_points)(g)\n",
        "  global_feature = Lambda(exp_dim, arguments={'num_points': num_points})(global_feature)\n",
        "\n",
        "  conc = concatenate([seg_part1, global_feature])\n",
        "\n",
        "  model = Model(inputs = input_points, outputs = conc, name=\"submodel\")\n",
        "  model.load_weights('/content/drive/My Drive/PointNet/submodel.h5')\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jbtTJoJpBek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "610693ef-3ab8-4be2-c051-83b512679427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i94JOrS3UagU"
      },
      "source": [
        "Pendant ce temps, vous avez astucieusement utilisé votre temps et votre moteur de recherche favori, ce qui vous a permis de trouver le code de deux fonctions utilisées pour réaliser de l'augmentation de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12U38_PkSeOj"
      },
      "outputs": [],
      "source": [
        "# Fonction auxiliaire de rotation des points\n",
        "def rotate_point_cloud(batch_data):\n",
        "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    for k in range(batch_data.shape[0]):\n",
        "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
        "        cosval = np.cos(rotation_angle)\n",
        "        sinval = np.sin(rotation_angle)\n",
        "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
        "                                    [0, 1, 0],\n",
        "                                    [-sinval, 0, cosval]])\n",
        "        shape_pc = batch_data[k, ...]\n",
        "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
        "    return rotated_data\n",
        "\n",
        "# Fonction auxiliaire de bruitage des points\n",
        "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
        "    B, N, C = batch_data.shape\n",
        "    assert(clip > 0)\n",
        "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\n",
        "    jittered_data += batch_data\n",
        "    return jittered_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LWUNe5-SgXM"
      },
      "source": [
        "\n",
        "\n",
        "La seule tâche restant à réaliser est le réseau de segmentation (i.e. la partie avec le fond jaune), et c'est votre projet ! Toute votre équipe et toute votre entreprise compte sur vous !\n",
        "\n",
        "Le réseau que vous devez implémenter doit donc prendre en entrée les prédictions du réseau précédent et déterminer la classe à laquelle appartient chaque point du nuage de points. \n",
        "\n",
        "Il n'est pas demandé d'entraîner le réseau fourni par votre collègue, l'apprentissage ayant déjà été effectué auparavant, mais seulement d'entraîner le réseau que vous allez implémenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl2KrUrQTCLr"
      },
      "outputs": [],
      "source": [
        "# Ne pas oublier d'importer les bons élements de la bibliothèse Keras\n",
        "\n",
        "# Construction du modèle pour la segmentation\n",
        "\"\"\"on implémente la partie jaune. get submodel permet d'obtenir la concaténation à l'entrée du modele\n",
        " mais on la fout pas ici mais après là on se préoccupe juste des dimensions.\"\"\"\n",
        "def segmenter(input_shape=(num_points,1088)):\n",
        "    input_points = Input(shape=input_shape)\n",
        "    # MLP(512,256)\n",
        "    # à implementer \n",
        "\n",
        "\n",
        "    # MLP(128,m)\n",
        "    # à implementer \n",
        "    \n",
        "    model = ...\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre d'epochs\n",
        "epo = 8\n",
        "\n",
        "# Chargement du modèle de classification \n",
        "submodel = ...\n",
        "\n",
        "# Construction des données de test\n",
        "test_points_segmenter = ..."
      ],
      "metadata": {
        "id": "9AA_S7npHCYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeXLlYYDTvBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "a8e67308-f420-4b48-b124-8181a88bfc74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-35ee42119b58>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    print('Test accuracy: ', score[1])\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Compilation du modèle\n",
        "model = segmenter()\n",
        "model.summary()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Sauvegarder du meilleur model <=> Celui qui a la loss la plus petite\n",
        "# Initialisation\n",
        "best_model = ...\n",
        "best_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "best_score_loss = 100\n",
        "\n",
        "for i in range(epo):\n",
        "    # Rotation et bruitage des points\n",
        "    # à implementer \n",
        "\n",
        "    # Construction des données\n",
        "    latent = ...\n",
        "\n",
        "    # Entraînement\n",
        "    # à implementer \n",
        "\n",
        "    # Pour des questions de RAM\n",
        "    del latent\n",
        "\n",
        "    # Évaluation du modèle & sauvegarde du meilleur model\n",
        "    score = model.evaluate(test_points_segmenter, test_labels, verbose=1)\n",
        "    print('Test loss: ', score[0])\n",
        "    if score[0] < best_score_loss:\n",
        "        # à implementer \n",
        "        \n",
        "    print('Test accuracy: ', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1mj9QKWAcJe"
      },
      "source": [
        "Comparaison entre le model après entrainement et le meilleur model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgxDymBKy-u4"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "score = model.evaluate(test_points_segmenter, test_labels, verbose=1)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "best_model.compile(optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "score = best_model.evaluate(test_points_segmenter, test_labels, verbose=1)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZA4y0sUTC0w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Visualisation\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "v_points = test_points[1:2,:,:]\n",
        "pred = model.predict(submodel.predict(v_points))\n",
        "pred = np.squeeze(pred)\n",
        "v_points = np.squeeze(v_points)\n",
        "pred = pred.tolist()\n",
        "color = ['b', 'g', 'r', 'k']\n",
        "m= ['o', 'v', '<', '>']\n",
        "for i in range(v_points.shape[0]):\n",
        "    xs = v_points[i,0]\n",
        "    ys = v_points[i,1]\n",
        "    zs = v_points[i,2]\n",
        "    ind = pred[i].index(max(pred[i]))\n",
        "    ax.scatter(xs, ys, zs, c=color[ind], marker=m[ind])\n",
        "  \n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(-1,1)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TP_PointNet_2022_etudiant (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}